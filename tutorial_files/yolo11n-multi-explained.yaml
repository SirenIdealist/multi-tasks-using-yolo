# 多任务YOLO配置详解
# ===================

# 这是一个简化的多任务YOLO配置文件，用于教学目的
# 每个部分都有详细的中文注释

# 基本参数配置
# ============
nc: 80  # 检测任务的类别数量（比如COCO数据集有80个类别：人、车、猫、狗等）
kpt_shape: [17, 3]  # 姿态估计任务：17个人体关键点，每个点有3个值(x坐标, y坐标, 可见性)
channels: 3  # 输入图片的通道数：RGB彩色图片=3，灰度图片=1

# 模型大小配置（类似于衣服的S、M、L、XL码）
# =========================================
scales:
  # 格式：[深度倍数, 宽度倍数, 最大通道数]
  n: [0.50, 0.25, 1024]  # nano版本：最小最快，适合手机等设备
  s: [0.50, 0.50, 1024]  # small版本：小而准确，适合边缘设备
  m: [0.50, 1.00, 512]   # medium版本：平衡版本，适合大多数应用
  l: [1.00, 1.00, 512]   # large版本：准确但较慢，适合服务器
  x: [1.00, 1.50, 512]   # extra large版本：最准确但最慢，适合离线处理

# Backbone（骨干网络）配置
# =======================
# 这部分负责从原始图片中提取基础特征
# 就像人的眼睛，先识别边缘、颜色、纹理等基本信息

backbone:
  # 配置格式：[输入来源, 重复次数, 模块类型, [参数列表]]
  
  # 第一阶段：初步特征提取
  - [-1, 1, Conv, [64, 3, 2]]      # 卷积层：输入→64通道，3×3卷积核，步长2（图片缩小一半）
  - [-1, 1, Conv, [128, 3, 2]]     # 卷积层：64→128通道，继续缩小图片
  
  # 第二阶段：深层特征提取
  - [-1, 2, C3k2, [256, False, 0.25]]  # C3k2模块：特殊的卷积块，重复2次，输出256通道
  - [-1, 1, Conv, [256, 3, 2]]     # 继续下采样
  - [-1, 2, C3k2, [512, False, 0.25]]  # 继续提取更深层的特征
  
  # 第三阶段：高级特征提取
  - [-1, 1, Conv, [512, 3, 2]]     # P4层：适合检测中等大小的物体
  - [-1, 2, C3k2, [512, True]]     
  - [-1, 1, Conv, [1024, 3, 2]]    # P5层：适合检测大物体
  - [-1, 2, C3k2, [1024, True]]
  
  # 特征增强层
  - [-1, 1, SPPF, [1024, 5]]       # 空间金字塔池化：增强多尺度特征
  - [-1, 2, C2PSA, [1024]]         # 位置自注意力：增强空间信息

# Head（头部网络）配置  
# ==================
# 这部分包含Neck（特征融合）和多个任务头

head:
  # === Neck部分：特征融合网络 ===
  # 目的：融合不同层次的特征，让模型既能检测大物体又能检测小物体
  
  # 上采样路径：从深层特征到浅层特征
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]  # 上采样：放大特征图
  - [[-1, 6], 1, Concat, [1]]       # 特征拼接：把当前层和第6层(P4)拼接
  - [-1, 2, C3k2, [512, False]]     # 特征融合：混合拼接的特征
  
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]  # 再次上采样
  - [[-1, 4], 1, Concat, [1]]       # 与第4层(P3)拼接
  - [-1, 2, C3k2, [256, False]]     # P3层融合特征：用于检测小物体

  # 下采样路径：从浅层特征到深层特征  
  - [-1, 1, Conv, [256, 3, 2]]      # 下采样
  - [[-1, 13], 1, Concat, [1]]      # 特征拼接
  - [-1, 2, C3k2, [512, False]]     # P4层融合特征：用于检测中等物体

  - [-1, 1, Conv, [512, 3, 2]]      # 下采样
  - [[-1, 10], 1, Concat, [1]]      # 特征拼接  
  - [-1, 2, C3k2, [1024, True]]     # P5层融合特征：用于检测大物体

  # === 多任务头部分 ===
  # 每个头负责一个特定任务，使用相同的融合特征但做不同的预测
  
  # 检测头：预测物体的边界框和类别
  - [[16, 19, 22], 1, Detect, [nc]]
  # 解释：使用第16、19、22层（对应P3、P4、P5）的特征
  #      Detect模块负责预测：边界框坐标(x,y,w,h) + 物体类别概率
  
  # 分割头：预测物体的像素级掩码
  - [[16, 19, 22], 1, Segment, [nc, 32, 256]]
  # 解释：除了边界框和类别，还预测每个像素是否属于某个物体
  #      32: 掩码原型数量, 256: 掩码特征维度
  
  # 姿态估计头：预测人体关键点
  - [[16, 19, 22], 1, Pose, [nc, kpt_shape]]
  # 解释：预测人体17个关键点的位置（头、肩膀、手肘、膝盖等）
  #      每个关键点有(x,y,可见性)三个值

# ===============================================
# 模型工作流程总结
# ===============================================

# 1. 输入处理：
#    原始图片(640×640×3) → Backbone特征提取

# 2. 特征提取：
#    Backbone提取多层次特征：
#    - P3 (80×80): 适合小物体
#    - P4 (40×40): 适合中等物体  
#    - P5 (20×20): 适合大物体

# 3. 特征融合：
#    Neck网络融合不同层次的特征，增强检测能力

# 4. 多任务预测：
#    三个头部同时工作：
#    - Detect头 → 边界框 + 类别
#    - Segment头 → 边界框 + 类别 + 分割掩码
#    - Pose头 → 边界框 + 类别 + 关键点位置

# 5. 输出格式：
#    {
#        'Detect': [P3检测结果, P4检测结果, P5检测结果],
#        'Segment': [P3分割结果, P4分割结果, P5分割结果], 
#        'Pose': [P3姿态结果, P4姿态结果, P5姿态结果]
#    }

# ===============================================
# 配置要点提醒
# ===============================================

# 1. 修改类别数量：
#    - 如果你的数据集不是80类，修改 nc 参数
#    - 比如只检测人和车：nc: 2

# 2. 修改关键点数量：
#    - 如果不是人体姿态，修改 kpt_shape
#    - 比如手部21个关键点：kpt_shape: [21, 3]

# 3. 选择模型大小：
#    - 手机APP用nano版本 (n)
#    - 实时应用用small版本 (s)  
#    - 离线处理用large版本 (l)

# 4. 任务选择：
#    - 如果不需要某个任务，可以删除对应的头
#    - 比如只要检测和分割，删除Pose头那一行

# 5. 输入尺寸：
#    - 这个配置适用于640×640输入
#    - 如果要用其他尺寸，可能需要调整某些参数
